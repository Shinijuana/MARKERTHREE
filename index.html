<html>
  <head>
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://aframe.io/releases/1.5.0/aframe.min.js"></script>
    <script src="https://cdn.jsdelivr.net/npm/mind-ar@1.2.5/dist/mindar-image-aframe.prod.js"></script>
</head>
<body>
  <!-- Overlay di scansione per il tracciamento dell'image target -->

  <!-- Configurazione di A-Frame con MindAR -->
  <a-scene mindar-image="uiScanning: no; imageTargetSrc: https://raw.githack.com/Shinijuana/MARKERTHREE/main/dos.mind; filterMinCF:0.0001; filterBeta: 100; warmupTolerance: 1; missTolerance: 1;" 
           tensorflow-contour-processor
           color-space="sRGB" 
           renderer="colorManagement: true, physicallyCorrectLights" 
           vr-mode-ui="enabled: false" 
           embedded
           device-orientation-permission-ui="enabled: false">
    
    <!-- Caricamento degli asset -->
    <a-assets>
      <a-asset-item id="avatarModel" src="https://raw.githack.com/Shinijuana/MARKERTHREE/main/assets/busto%20emilio.gltf"></a-asset-item>
    </a-assets>

    <!-- Configurazione della telecamera -->
    <a-camera position="0 0 0" look-controls="enabled: false" cursor="fuse: false; rayOrigin: mouse;" raycaster="near: 10; far: 10000; objects: .clickable">
      <a-image
      id="startImage"
      src="assets/markerdos.jpg"
      position="0 0 -15"
      scale="10 7 10"
      opacity=".8"
      visible="true"
      animation__pulse="property: scale; from: 5 5 5; to: 5.5 5.5 5.5; dir: alternate; loop: true; dur: 1000">
    </a-image>
    </a-camera>

    <!-- Entità per il target AR e il modello 3D -->
    <a-entity id="example-target" mindar-image-target="targetIndex: 0" keepvisibleonlost>
      <a-entity
        id="avatarModelEntity"
        rotation="90 0 0" 
        position="0 0 0" 
        scale="2 2 2" 
        gltf-model="#avatarModel"
        class="clickable"
        animation__move="property: position; from: 0 0 0.5; to: 0 0 .9; dur: 1680; loop: true; dir: alternate; easing: easeInOutSine;">
      </a-entity>
    </a-entity>

</a-scene>

    <script>
AFRAME.registerComponent('keepvisibleonlost', {
  init() {
    const el = this.el;
    const startImage = document.getElementById('startImage');
    const viewmodel = document.getElementById('avatarModelEntity');

    // Inizialmente il modello è visibile
    el.object3D.visible = true;

    // Evento targetFound
    el.sceneEl.addEventListener('targetFound', (event) => {
      console.log('Target found');
      if (startImage) startImage.setAttribute('visible', 'false');
      if (viewmodel) viewmodel.setAttribute('visible', 'true');
      el.object3D.visible = true;
    });

    // Evento targetLost
    el.sceneEl.addEventListener('targetLost', (event) => {
      console.log('Target lost');
      // Non nascondere il modello, ma mantenerlo visibile
      el.object3D.visible = true;
    });
  },
});
</script>
  <script>
    // Componente A-Frame
AFRAME.registerComponent('tensorflow-contour-processor', {
  schema: {
    targetName: {type: 'string'},
  },

  init() {
    // Carica OpenCV.js
    this.loadOpenCV()
  },

  loadOpenCV() {
    // Aggiungi messaggio di debug
    console.log('Loading OpenCV.js...')

    // Carica OpenCV.js se non è già stato caricato
    if (!window.cv) {
      const opencvScript = document.createElement('script')
      opencvScript.src = 'https://docs.opencv.org/master/opencv.js'
      opencvScript.async = true
      opencvScript.onload = () => {
        console.log('OpenCV.js script loaded.')
      }
      opencvScript.onerror = () => {
        console.error('Failed to load OpenCV.js script.')
      }
      document.head.appendChild(opencvScript)
    } else {
      // Se OpenCV.js è già stato caricato, chiama direttamente l'inizializzazione
      console.log('OpenCV.js is already loaded.')
      onOpenCvReady()
    }
  },

  initializeWebcamAndCanvas() {
    const video = document.createElement('video')
    video.setAttribute('autoplay', '')
    document.body.appendChild(video)

    const canvas = document.createElement('canvas')
    document.body.appendChild(canvas)
    const ctx = canvas.getContext('2d')

    // Messaggio di debug
    console.log('Initializing webcam...')

    navigator.mediaDevices.getUserMedia({video: true})
      .then((stream) => {
        video.srcObject = stream
        video.onloadedmetadata = () => {
          canvas.width = video.videoWidth
          canvas.height = video.videoHeight

          // Messaggio di debug
          console.log('Webcam initialized. Video dimensions:', video.videoWidth, video.videoHeight)

          // Loop per il rilevamento
          const processFrame = () => {
            ctx.drawImage(video, 0, 0)
            const src = cv.imread(canvas)
            const dst = new cv.Mat()
            cv.cvtColor(src, src, cv.COLOR_RGBA2GRAY)
            cv.Canny(src, dst, 50, 100)
            cv.imshow(canvas, dst)

            // Messaggi di debug
            console.log('Frame processed.')
            console.log('Source matrix shape:', src.rows, src.cols)
            console.log('Destination matrix shape:', dst.rows, dst.cols)

            src.delete()
            dst.delete()
            requestAnimationFrame(processFrame)
          }
          requestAnimationFrame(processFrame)
        }
      })
      .catch((err) => {
        console.error('Error accessing webcam:', err)
      })
  },

  remove() {
    // Pulizia
    const video = document.querySelector('video')
    const canvas = document.querySelector('canvas')

    if (video) {
      if (!video.paused) {
        video.pause()
      }
      if (video.srcObject) {
        video.srcObject.getTracks().forEach(track => track.stop())
      }
      document.body.removeChild(video)
    }

    if (canvas) {
      document.body.removeChild(canvas)
    }

    // Messaggio di debug
    console.log('Cleanup complete.')
  },
})
  </script>
</body>
</html>
